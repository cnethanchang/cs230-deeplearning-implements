{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append( os.path.abspath(\"../../workspace/python-work/python 3/cs230_deeplearning_implement/ec_code/\")  )\n",
    "from class4.week1.assignment.cnn_utils import *\n",
    "\n",
    "#如果在ipython里已经import过的模块修改后需要重新reload就需要在执行用户代码前，重新装入软件的扩展和模块。\n",
    "#autoreload 2：装入所有 %aimport 不包含的模块\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: zero_pad\n",
    "def zero_pad(X, pad):\n",
    "    X_pad=np.pad(X,(\n",
    "                        (0,0),       #样本数，不填充\n",
    "                        (pad,pad),   #图像高度,你可以视为上面填充x个，下面填充y个(x,y)\n",
    "                        (pad,pad),   #图像宽度,你可以视为左边填充x个，右边填充y个(x,y)\n",
    "                        (0,0)),      #通道数，不填充\n",
    "                        'constant', constant_values=0)      #连续一样的值填充\n",
    "    return X_pad\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: conv_single_step    单步卷积\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    # Element-wise product between a_slice and W. Add bias.\n",
    "    s = np.multiply(a_slice_prev, W) + b\n",
    "    # Sum over all entries of the volume s\n",
    "    Z = np.sum(s)\n",
    "    return Z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: conv_forward\n",
    "    # Implements the forward propagation for a convolution function\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    A_prev -- shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- shape (f, f, n_C_prev, n_C)\n",
    "    b --  (1, 1, 1, n_C)\n",
    "    hparameters -- dictionary containing \"stride\" and \"pad\"\n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "   #=======================important============================== \n",
    "    #=======================important============================== \n",
    "    #=======================important============================== \n",
    "    n_H =int((n_H_prev + 2 * pad - f) / stride)+1\n",
    "    n_W =int((n_W_prev + 2 * pad - f) / stride)+1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros.\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)    #A_prev.shape=>  (m,n_h,n_w,n_c)\n",
    "    \n",
    "    for i in range(m):                               # loop over the batch of training examples\n",
    "        a_prev_pad=A_prev_pad[i]   # Select ith training example's padded activation\n",
    "        for h in range(n_H):          # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):      # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):     # loop over channels (= #filters) of the output volume\n",
    "                    #定位当前的切片位置\n",
    "                    vert_start=h * stride\n",
    "                    vert_end=vert_start + f\n",
    "                    horiz_start=w * stride\n",
    "                    horiz_end=horiz_start + f\n",
    "                    #自行脑补一下吸管插入一层层的橡皮泥就明白了\n",
    "                    #===========================important====================================\n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell)\n",
    "                    a_slice_prev=a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron.\n",
    "                    Z[i, h, w, c]=np.sum(  np.multiply(a_slice_prev, W[:, :, :, c])    + b[:, :, :, c]  )\n",
    "    \n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: pool_forward\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    A_prev -- shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- defined as a string (\"max\" or \"average\")\n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.608990675868995\ndW_mean = 10.581741275547566\ndb_mean = 76.37106919563735\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#卷积层的反向传播\n",
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        dZ - 卷积层的输出Z的 梯度，维度为(m, n_H, n_W, n_C)\n",
    "        cache - 反向传播所需要的参数，conv_forward()的输出之一\n",
    "\n",
    "    返回：\n",
    "        dA_prev - 卷积层的输入（A_prev）的梯度值，维度为(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        dW - 卷积层的权值的梯度，维度为(f,f,n_C_prev,n_C)\n",
    "        db - 卷积层的偏置的梯度，维度为（1,1,1,n_C）\n",
    "    \"\"\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    #前向传播中我们使用了pad，反向传播也需要使用，这是为了保证数据结构一致\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    #定位完毕，开始切片\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    # ==============================important=============================\n",
    "                    # ==============================important=============================\n",
    "                    # ==============================important=============================\n",
    "                    # ==============================important=============================\n",
    "                    # ==============================important=============================\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    # nn  ===>  dA=W.T * dZ\n",
    "                    #cnn=====>  dA +=   ∑_n_H  ∑_n_W(W_c*dZ)\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "       #设置第i个样本最终的dA_prev,即把非填充的数据取出来 \n",
    "        dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "#初始化参数\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2, \"stride\": 1}\n",
    "\n",
    "#前向传播\n",
    "Z , cache_conv = conv_forward(A_prev,W,b,hparameters)\n",
    "#反向传播\n",
    "dA , dW , db = conv_backward(Z,cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
