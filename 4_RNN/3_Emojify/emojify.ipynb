{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append( os.path.abspath(\"../../workspace/python-work/python 3/cs230_deeplearning_implement/ec_code/\")  )\n",
    "from class5.week2.Emojify.emo_utils import *\n",
    "\n",
    "import numpy as np\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Emojiè¡¨æƒ…ç”Ÿæˆå™¨\n",
    "# æ¯”å¦‚å†™ä¸‹â€œCongratulations on the promotion! Lets get coffee and talk. Love you!â€\n",
    "# ï¼Œé‚£ä¹ˆä½ çš„è¡¨æƒ…ç”Ÿæˆå™¨å°±ä¼šè‡ªåŠ¨ç”Ÿæˆ\n",
    "# â€œ Congratulations on the promotion! ğŸ‘ Lets get coffee and talk. â˜•ï¸ Love you! â¤ï¸â€\n",
    "\n",
    "\n",
    "#1 - åŸºå‡†æ¨¡å‹ï¼šEmojifier-V1     \n",
    "#    å¯¹äºæ¯ä¸ªsentenceï¼Œæ±‚å‡ºå¤šä¸ªwordçš„avg(50,1)ï¼Œæ±‚å‡ºå¯¹åº”çš„wï¼ˆ5,50ï¼‰  æ ¹æ®y(5,m)è®­ç»ƒ\n",
    "#       Emojifier-V1æ˜¯æœ‰ç¼ºé™·çš„ï¼Œæ¯”å¦‚å®ƒä¸ä¼šæŠŠâ€œyou are not happyâ€åˆ’åˆ†ä¸ºä¸å¥½ä¸€ç±»ï¼Œ\n",
    "#=======å› ä¸ºå®ƒåªæ˜¯å°†æ‰€æœ‰å•è¯çš„å‘é‡åšäº†å¹³å‡ï¼Œæ²¡æœ‰å…³å¿ƒè¿‡é¡ºåº\n",
    "#   1.1 æ•°æ®é›†\n",
    "#   1.2 - Emojifier-V1çš„ç»“æ„\n",
    "#   1.3 - å®ç°Emojifier-V1\n",
    "#   1.4 - éªŒè¯æµ‹è¯•é›†\n",
    "\n",
    "\n",
    "# 2 - Emojifier-V2ï¼šåœ¨Kerasä¸­ä½¿ç”¨LSTMæ¨¡å—\n",
    "#   2.1 - æ¨¡å‹é¢„è§ˆ\n",
    "#   2.2 - Kerasä¸mini-batching\n",
    "#   2.3 - åµŒå…¥å±‚ï¼ˆ The Embedding layerï¼‰\n",
    "#   4.4 - æ„å»ºEmojifier-V2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\nI am so impressed by your dedication to this project\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = read_csv(r'C:\\workspace\\python-work\\python 3\\cs230_deeplearning_implement\\ec_code\\class5\\week2\\Emojify\\data\\train_emoji.csv')\n",
    "X_test, Y_test = read_csv(r'C:\\workspace\\python-work\\python 3\\cs230_deeplearning_implement\\ec_code\\class5\\week2\\Emojify\\data\\tesss.csv')\n",
    "maxLen = len(   max(X_train, key=len).split()    )\n",
    "print(maxLen)\n",
    "print(max(X_train,key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss you so much â¤ï¸\n<class 'numpy.ndarray'>\nlength= 5\n0 ==> â¤ï¸\n1 ==> âš¾\n2 ==> ğŸ˜„\n3 ==> ğŸ˜\n4 ==> ğŸ´\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "print(X_train[index], label_to_emoji(Y_train[index]))\n",
    "\n",
    "\n",
    "print(type(Y_train))\n",
    "print(\"length=\",len(set(Y_train)))\n",
    "y_train_set=set(Y_train)\n",
    "\n",
    "for y in y_train_set:\n",
    "    print(y,\"==>\",label_to_emoji(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 is converted into one hot [0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Emojifier-V1çš„ç»“æ„\n",
    "#æ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€æ®µæ–‡å­—ï¼ˆæ¯”å¦‚â€œl lov youâ€ï¼‰ï¼Œè¾“å‡ºçš„æ˜¯ç»´åº¦ä¸º(1,5)çš„å‘é‡ï¼Œ\n",
    "# æœ€ååœ¨argmaxå±‚æ‰¾å¯»æœ€å¤§å¯èƒ½æ€§çš„è¾“å‡ºã€‚\n",
    "\n",
    "\n",
    "#ç°åœ¨å°†æ ‡ç­¾Yè½¬æ¢æˆsoftmaxåˆ†ç±»å™¨æ‰€éœ€è¦çš„æ ¼å¼ï¼Œå³ä»(m,1)è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç (m,5)ï¼Œ\n",
    "# æ¯ä¸€è¡Œéƒ½æ˜¯ç»è¿‡ç¼–ç åçš„æ ·æœ¬ï¼Œ å…¶ä¸­Y_ohæŒ‡çš„æ˜¯â€œY-one-hotâ€ã€‚\n",
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)\n",
    "\n",
    "index = 2\n",
    "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.9520498812810072\nAccuracy: 0.3484848484848485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 --- cost = 0.07971818726014807\nAccuracy: 0.9318181818181818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 --- cost = 0.04456369243681402\nAccuracy: 0.9545454545454546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 --- cost = 0.03432267378786059\nAccuracy: 0.9696969696969697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\nAccuracy: 0.9772727272727273\nTest set:\nAccuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map =read_glove_vecs(r\"C:\\workspace\\python-work\\python 3\\cs230_deeplearning_implement\\ec_code\\class5\\week2\\Emojify\\data\\glove6B50d.txt\")\n",
    "#word_to_indexï¼šå­—å…¸ç±»å‹çš„è¯æ±‡ï¼ˆ400,001ä¸ªï¼‰ä¸ç´¢å¼•çš„æ˜ å°„ï¼ˆæœ‰æ•ˆèŒƒå›´ï¼š0-400,000ï¼‰\n",
    "# index_to_wordï¼šå­—å…¸ç±»å‹çš„ç´¢å¼•ä¸è¯æ±‡ä¹‹é—´çš„æ˜ å°„ã€‚\n",
    "# word_to_vec_mapï¼šå­—å…¸ç±»å‹çš„è¯æ±‡ä¸å¯¹åº”GloVeå‘é‡çš„æ˜ å°„ã€‚\n",
    "\n",
    "word = \"cucumber\"\n",
    "index = 113317\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.9520498812810072\nAccuracy: 0.3484848484848485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 --- cost = 0.07971818726014807\nAccuracy: 0.9318181818181818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 --- cost = 0.04456369243681402\nAccuracy: 0.9545454545454546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 --- cost = 0.03432267378786059\nAccuracy: 0.9696969696969697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\nAccuracy: 0.9772727272727273\nTest set:\nAccuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: sentence_to_avg\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    ''' \n",
    "   å°†å¥å­è½¬æ¢ä¸ºå•è¯åˆ—è¡¨ï¼Œæå–å…¶GloVeå‘é‡ï¼Œç„¶åå°†å…¶å¹³å‡ã€‚\n",
    "    å‚æ•°ï¼š\n",
    "        sentence -- å­—ç¬¦ä¸²ç±»å‹ï¼Œä»Xä¸­è·å–çš„æ ·æœ¬ã€‚\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹ï¼Œå•è¯æ˜ å°„åˆ°50ç»´çš„å‘é‡çš„å­—å…¸\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        avg -- å¯¹å¥å­çš„å‡å€¼ç¼–ç ï¼Œç»´åº¦ä¸º(50,)\n",
    "    '''    \n",
    "    # ç¬¬ä¸€æ­¥ï¼šåˆ†å‰²å¥å­ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨ã€‚\n",
    "    words = sentence.lower().split()\n",
    "    # åˆå§‹åŒ–å‡å€¼è¯å‘é‡\n",
    "    avg = np.zeros(50,)\n",
    "    # ç¬¬äºŒæ­¥ï¼šå¯¹è¯å‘é‡å–å¹³å‡ã€‚\n",
    "    for w in words:\n",
    "        avg += word_to_vec_map[w]\n",
    "    avg = np.divide(avg, len(words))\n",
    "    return avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: model\n",
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \"\"\"\n",
    "    åœ¨numpyä¸­è®­ç»ƒè¯å‘é‡æ¨¡å‹ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        X -- è¾“å…¥çš„å­—ç¬¦ä¸²ç±»å‹çš„æ•°æ®ï¼Œç»´åº¦ä¸º(m, 1)ã€‚\n",
    "        Y -- å¯¹åº”çš„æ ‡ç­¾ï¼Œ0-7çš„æ•°ç»„ï¼Œç»´åº¦ä¸º(m, 1)ã€‚\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹çš„å•è¯åˆ°50ç»´è¯å‘é‡çš„æ˜ å°„ã€‚\n",
    "        learning_rate -- å­¦ä¹ ç‡.\n",
    "        num_iterations -- è¿­ä»£æ¬¡æ•°ã€‚\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        pred -- é¢„æµ‹çš„å‘é‡ï¼Œç»´åº¦ä¸º(m, 1)ã€‚\n",
    "        W -- æƒé‡å‚æ•°ï¼Œç»´åº¦ä¸º(n_y, n_h)ã€‚\n",
    "        b -- åç½®å‚æ•°ï¼Œç»´åº¦ä¸º(n_y,)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Define number of training examples\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = 5                                 # number of classes  \n",
    "    n_h = 50                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
    "        for i in range(m):                                # Loop over the training examples\n",
    "            \n",
    "            # è·å–ç¬¬iä¸ªè®­ç»ƒæ ·æœ¬çš„å‡å€¼\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "            \n",
    "            # å‰å‘ä¼ æ’­\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "            \n",
    "            # è®¡ç®—ç¬¬iä¸ªè®­ç»ƒçš„æŸå¤±\n",
    "            cost = -np.sum(Y_oh[i]*np.log(a))\n",
    "            \n",
    "            # è®¡ç®—æ¢¯åº¦\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "            \n",
    "            # æ›´æ–°å‚æ•°\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "    return pred, W, b\n",
    "\n",
    "\n",
    "\n",
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "\n",
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n\ni adore you â¤ï¸\nyou are happy â¤ï¸\nyou are not happy â¤ï¸\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"you are happy\",  \"you are not happy\"])\n",
    "Y_my_labels = np.array([[0], [0],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 10)                0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 10, 50)            20000050  \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 10, 128)           91648     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 10, 128)           0         \n_________________________________________________________________\nlstm_4 (LSTM)                (None, 128)               131584    \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 645       \n_________________________________________________________________\nactivation_2 (Activation)    (None, 5)                 0         \n=================================================================\nTotal params: 20,223,927\nTrainable params: 223,877\nNon-trainable params: 20,000,050\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Emojifier-V2ï¼šåœ¨Kerasä¸­ä½¿ç”¨LSTMæ¨¡å—\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(1)\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 10)                0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 10, 50)            20000050  \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 10, 128)           91648     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 10, 128)           0         \n_________________________________________________________________\nlstm_4 (LSTM)                (None, 128)               131584    \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 645       \n_________________________________________________________________\nactivation_2 (Activation)    (None, 5)                 0         \n=================================================================\nTotal params: 20,223,927\nTrainable params: 223,877\nNon-trainable params: 20,000,050\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Kerasä¸mini-batching\n",
    "# åœ¨è¿™ä¸ªéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨mini-batchesæ¥è®­ç»ƒKerasæ¨¡å‹ï¼Œ\n",
    "# ä½†æ˜¯å¤§éƒ¨åˆ†æ·±åº¦å­¦ä¹ æ¡†æ¶éœ€è¦ä½¿ç”¨ç›¸åŒçš„é•¿åº¦çš„æ–‡å­—ï¼Œè¿™æ˜¯å› ä¸ºå¦‚æœä½ ä½¿ç”¨3ä¸ªå•è¯ä¸4ä¸ªå•è¯çš„å¥å­ï¼Œ\n",
    "# é‚£ä¹ˆè½¬åŒ–ä¸ºå‘é‡ä¹‹åï¼Œè®¡ç®—æ­¥éª¤å°±æœ‰æ‰€ä¸åŒï¼ˆä¸€ä¸ªæ˜¯éœ€è¦3ä¸ªLSTMï¼Œå¦ä¸€ä¸ªéœ€è¦4ä¸ªLSTMï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸å¯èƒ½å¯¹è¿™äº›å¥å­è¿›è¡ŒåŒæ—¶è®­ç»ƒã€‚\n",
    "# \n",
    "# é‚£ä¹ˆé€šç”¨çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨å¡«å……ã€‚æŒ‡å®šæœ€é•¿å¥å­çš„é•¿åº¦ï¼Œç„¶åå¯¹å…¶ä»–å¥å­è¿›è¡Œå¡«å……åˆ°ç›¸åŒé•¿åº¦ã€‚\n",
    "# æ¯”å¦‚ï¼šæŒ‡å®šæœ€å¤§çš„å¥å­çš„é•¿åº¦ä¸º20ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ¯ä¸ªå¥å­ä½¿ç”¨â€œ0â€æ¥å¡«å……ï¼Œç›´åˆ°å¥å­é•¿åº¦ä¸º20ï¼Œ\n",
    "# å› æ­¤ï¼Œå¥å­â€œI love youâ€å°±å¯ä»¥è¡¨ç¤ºä¸º(eI,elove,eyou,0âƒ—Â ,0âƒ—Â ,â€¦,0âƒ—Â )\n",
    "# åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä»»ä½•ä»»ä½•ä¸€ä¸ªè¶…è¿‡20ä¸ªå•è¯çš„å¥å­å°†è¢«æˆªå–ï¼Œæ‰€ä»¥ä¸€ä¸ªæ¯”è¾ƒç®€å•çš„æ–¹å¼å°±æ˜¯æ‰¾åˆ°æœ€é•¿å¥å­ï¼Œè·å–å®ƒçš„é•¿åº¦ï¼Œç„¶åæŒ‡å®šå®ƒçš„é•¿åº¦ä¸ºæœ€é•¿å¥å­çš„é•¿åº¦ã€‚\n",
    "# GRADED FUNCTION: sentences_to_indices\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    è¾“å…¥çš„æ˜¯Xï¼ˆå­—ç¬¦ä¸²ç±»å‹çš„å¥å­çš„æ•°ç»„ï¼‰ï¼Œå†è½¬åŒ–ä¸ºå¯¹åº”çš„å¥å­åˆ—è¡¨ï¼Œ\n",
    "    è¾“å‡ºçš„æ˜¯èƒ½å¤Ÿè®©Embedding()å‡½æ•°æ¥å—çš„åˆ—è¡¨æˆ–çŸ©é˜µï¼ˆå‚è§å›¾4ï¼‰ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        X -- å¥å­æ•°ç»„ï¼Œç»´åº¦ä¸º(m, 1)\n",
    "        word_to_index -- å­—å…¸ç±»å‹çš„å•è¯åˆ°ç´¢å¼•çš„æ˜ å°„\n",
    "        max_len -- æœ€å¤§å¥å­çš„é•¿åº¦ï¼Œæ•°æ®é›†ä¸­æ‰€æœ‰çš„å¥å­çš„é•¿åº¦éƒ½ä¸ä¼šè¶…è¿‡å®ƒã€‚\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        X_indices -- å¯¹åº”äºXä¸­çš„å•è¯ç´¢å¼•æ•°ç»„ï¼Œç»´åº¦ä¸º(m, max_len)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    # ä½¿ç”¨0åˆå§‹åŒ–X_indices\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        # å°†ç¬¬iä¸ªå±…ä½è½¬åŒ–ä¸ºå°å†™å¹¶æŒ‰å•è¯åˆ†å¼€ã€‚\n",
    "        sentences_words = X[i].lower().split()\n",
    "        # åˆå§‹åŒ–jä¸º0\n",
    "        j = 0\n",
    "        # éå†è¿™ä¸ªå•è¯åˆ—è¡¨\n",
    "        for w in sentences_words:\n",
    "            # å°†X_indicesçš„ç¬¬(i, j)å·å…ƒç´ ä¸ºå¯¹åº”çš„å•è¯ç´¢å¼•\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j += 1\n",
    "    return X_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: pretrained_embedding_layer\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºKeras Embedding()å±‚ï¼ŒåŠ è½½å·²ç»è®­ç»ƒå¥½äº†çš„50ç»´GloVeå‘é‡\n",
    "    å‚æ•°ï¼š\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹çš„å•è¯ä¸è¯åµŒå…¥çš„æ˜ å°„\n",
    "        word_to_index -- å­—å…¸ç±»å‹çš„å•è¯åˆ°è¯æ±‡è¡¨ï¼ˆ400,001ä¸ªå•è¯ï¼‰çš„ç´¢å¼•çš„æ˜ å°„ã€‚\n",
    "    è¿”å›ï¼š\n",
    "        embedding_layer() -- è®­ç»ƒå¥½äº†çš„Kerasçš„å®ä½“å±‚=========>ç›¸å½“äºå¾—åˆ°E,(40000,50)\n",
    "    \"\"\"\n",
    "    vocab_len = len(word_to_index) + 1    # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # =50  define dimensionality of your GloVe word vectors \n",
    "    \n",
    "    # åˆå§‹åŒ–åµŒå…¥çŸ©é˜µ\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # å°†åµŒå…¥çŸ©é˜µçš„æ¯è¡Œçš„â€œindexâ€è®¾ç½®ä¸ºè¯æ±‡â€œindexâ€çš„è¯å‘é‡è¡¨ç¤º\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    #========================================================\n",
    "    #========================================================\n",
    "    #========================================================\n",
    "    #========================================================\n",
    "    # å®šä¹‰Kerasçš„embbedingå±‚\n",
    "    #åœ¨Kerasä¸­å®šä¹‰åµŒå…¥å±‚ï¼Œå½“è°ƒç”¨Embedding()çš„æ—¶å€™éœ€è¦è®©è¿™ä¸€å±‚çš„å‚æ•°ä¸èƒ½è¢«è®­ç»ƒ\n",
    "    #æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è®¾ç½®trainable=Falseã€‚\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    # æ„å»ºembeddingå±‚\n",
    "    embedding_layer.build((None,))\n",
    "    # å°†åµŒå…¥å±‚çš„æƒé‡è®¾ç½®ä¸ºåµŒå…¥çŸ©é˜µã€‚\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: Emojify_V2\n",
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    æ„å»ºEmojifier-V2\n",
    "    å®ç°Emojify-V2æ¨¡å‹çš„è®¡ç®—å›¾\n",
    "    å‚æ•°ï¼š\n",
    "        input_shape -- è¾“å…¥çš„ç»´åº¦ï¼Œé€šå¸¸æ˜¯(max_len,)\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹çš„å•è¯ä¸è¯åµŒå…¥çš„æ˜ å°„ã€‚\n",
    "        word_to_index -- å­—å…¸ç±»å‹çš„å•è¯åˆ°è¯æ±‡è¡¨ï¼ˆ400,001ä¸ªå•è¯ï¼‰çš„ç´¢å¼•çš„æ˜ å°„ã€‚\n",
    "    è¿”å›ï¼š\n",
    "        model -- Kerasæ¨¡å‹å®ä½“\n",
    "    \"\"\"\n",
    "    # å®šä¹‰sentence_indicesä¸ºè®¡ç®—å›¾çš„è¾“å…¥ï¼Œç»´åº¦ä¸º(input_shape,)ï¼Œç±»å‹ä¸ºdtype 'int32'\n",
    "    # æ¨¡å‹çš„è¾“å…¥æ˜¯(m, max_len)ï¼Œå®šä¹‰åœ¨äº†input_shapeä¸­\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    # åˆ›å»ºembeddingå±‚,å¾—åˆ°E\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # é€šè¿‡åµŒå…¥å±‚ä¼ æ’­sentence_indicesï¼Œä½ ä¼šå¾—åˆ°åµŒå…¥çš„ç»“æœ====>(50ï¼Œm,max_len)\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # é€šè¿‡å¸¦æœ‰128ç»´éšè—çŠ¶æ€çš„LSTMå±‚ä¼ æ’­åµŒå…¥  n_a=128\n",
    "    # éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿”å›çš„è¾“å‡ºåº”è¯¥æ˜¯ä¸€æ‰¹åºåˆ—ã€‚\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    # ä½¿ç”¨dropoutï¼Œæ¦‚ç‡ä¸º0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # é€šè¿‡å¦ä¸€ä¸ª128ç»´éšè—çŠ¶æ€çš„LSTMå±‚ä¼ æ’­X\n",
    "    # æ³¨æ„ï¼Œè¿”å›çš„è¾“å‡ºåº”è¯¥æ˜¯å•ä¸ªéšè—çŠ¶æ€ï¼Œè€Œä¸æ˜¯ä¸€ç»„åºåˆ—===>åªæœ‰a,æ²¡æœ‰y\n",
    "    X = LSTM(128, return_sequences=False)(X)    #(n_a,m)\n",
    "    # ä½¿ç”¨dropoutï¼Œæ¦‚ç‡ä¸º0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # é€šè¿‡softmaxæ¿€æ´»çš„Denseå±‚ä¼ æ’­Xï¼Œå¾—åˆ°ä¸€æ‰¹5ç»´å‘é‡ã€‚\n",
    "    X = Dense(5)(X)  #ï¼ˆ5ï¼Œmï¼‰\n",
    "    # æ·»åŠ softmaxæ¿€æ´»\n",
    "    X = Activation('softmax')(X)     #(1,m)\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹å®ä½“\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = Emojify_V2(  (maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()\n",
    "#å¯ä»¥çœ‹åˆ°æœ‰â€œ20,223,927â€ä¸ªå‚æ•°ï¼Œå…¶ä¸­â€œ20,000,050â€ä¸ªå‚æ•°æ²¡æœ‰è¢«è®­ç»ƒï¼ˆè¿™æ˜¯å› ä¸ºå®ƒæ˜¯è¯å‘é‡ï¼‰\n",
    "#å‰©ä¸‹çš„æ˜¯æœ‰â€œ223,877â€è¢«è®­ç»ƒäº†çš„ã€‚å› ä¸ºæˆ‘ä»¬çš„å•è¯è¡¨æœ‰400,001ä¸ªå•è¯ï¼Œ\n",
    "# æ‰€ä»¥æ˜¯400,001âˆ—50=20,000,050 400,001*50=20,000,050400,001âˆ—50=20,000,050ä¸ªä¸å¯è®­ç»ƒçš„å‚æ•°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r32/56 [================>.............] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r56/56 [==============================] - 0s 892us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy =  0.8750000085149493\n"
     ]
    }
   ],
   "source": [
    "#ä½¿ç”¨categorical_crossentropy æŸå¤±, adam ä¼˜åŒ–å™¨ä¸ [â€˜accuracyâ€™] æŒ‡æ ‡ã€‚\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)\n",
    "\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r32/56 [================>.............] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r56/56 [==============================] - 0s 892us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy =  0.8750000085149493\n"
     ]
    }
   ],
   "source": [
    "#çœ‹åœ¨æµ‹è¯•é›†çš„è¡¨ç°\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print(\"Test accuracy = \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like this ğŸ˜„\n"
     ]
    }
   ],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "#çœ‹å“ªäº›è¢«åˆ†ç±»é”™è¯¯äº†\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like this ğŸ˜„\n"
     ]
    }
   ],
   "source": [
    "# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \n",
    "x_test = np.array(['i like this'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
