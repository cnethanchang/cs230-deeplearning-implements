{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dx\"][1][2] = [-2.07101689 -0.59255627  0.02466855  0.01483317]\ngradients[\"dx\"].shape = (3, 10, 4)\ngradients[\"da0\"][2][3] = -0.31494237512664996\ngradients[\"da0\"].shape = (5, 10)\ngradients[\"dWax\"][3][1] = 11.264104496527777\ngradients[\"dWax\"].shape = (5, 3)\ngradients[\"dWaa\"][1][2] = 2.303333126579893\ngradients[\"dWaa\"].shape = (5, 5)\ngradients[\"dba\"][4] = [-0.74747722]\ngradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append( os.path.abspath(\"../../workspace/python-work/python 3/cs230_deeplearning_implement/ec_code/\")  )\n",
    "from class5.week1.RecurrentNeuralNetworkStepbyStepv1.rnn_utils import *\n",
    "\n",
    "#一步步搭建循环神经网络\n",
    "# 1 - 循环神经网络的前向传播\n",
    "# 2 - 长短时记忆（Long Short-Term Memory (LSTM)）网络\n",
    "# 2.0.1 - 遗忘门\n",
    "# 2.0.2 - 更新门\n",
    "# 2.0.3 - 更新单元\n",
    "# 2.0.4 - 输出门\n",
    "# 2.2 - LSTM的前向传播\n",
    "# 3 - 循环神经网络的反向传播）\n",
    "# 3.1 - 基本的RNN网络的反向传播\n",
    "# 3.2 - LSTM反向传播\n",
    "# 3.2.1 - 单步反向传播\n",
    "# 3.2.2 门的导数\n",
    "# 3.2.3参数的导数\n",
    "# 3.3 - LSTM网络的反向传播\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dx\"][1][2] = [-2.07101689 -0.59255627  0.02466855  0.01483317]\ngradients[\"dx\"].shape = (3, 10, 4)\ngradients[\"da0\"][2][3] = -0.31494237512664996\ngradients[\"da0\"].shape = (5, 10)\ngradients[\"dWax\"][3][1] = 11.264104496527777\ngradients[\"dWax\"].shape = (5, 3)\ngradients[\"dWaa\"][1][2] = 2.303333126579893\ngradients[\"dWaa\"].shape = (5, 5)\ngradients[\"dba\"][4] = [-0.74747722]\ngradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \n",
    "#实现一个RNN单元，这需要由以下几步完成：\n",
    "#1 使用tanh函数计算隐藏单元的激活值：a⟨t⟩=tanh(W_aa * a⟨t−1⟩+W_ax * x⟨t⟩+ba)\n",
    "#2 使用a⟨t⟩计算                       yˆ⟨t⟩= softmax (W_ya* a⟨t⟩+by) \n",
    "#3把(a⟨t⟩,a_⟨t⟩−1,x⟨t⟩,parameters) 存储到cache中。\n",
    "#4返回a⟨t⟩,y⟨t⟩与cache。\n",
    "def rnn_cell_forward(x_t, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell \n",
    "    参数：\n",
    "        xt -- 时间步“t”输入的数据，维度为（n_x, m）\n",
    "        a_prev -- 时间步“t - 1”的隐藏隐藏状态，维度为（n_a, m）\n",
    "        parameters -- 字典，包含了以下内容:\n",
    "                        Wax -- 矩阵，输入乘以权重，维度为（n_a, n_x）\n",
    "                        Waa -- 矩阵，隐藏状态乘以权重，维度为（n_a, n_a）\n",
    "                        Wya -- 矩阵，隐藏状态与输出相关的权重矩阵，维度为（n_y, n_a）\n",
    "                        ba  -- 偏置，维度为（n_a, 1）\n",
    "                        by  -- 偏置，隐藏状态与输出相关的偏置，维度为（n_y, 1）\n",
    "    \n",
    "    返回：\n",
    "        a_next -- 下一个隐藏状态，维度为（n_a， m）\n",
    "        yt_pred -- 在时间步“t”的预测，维度为（n_y， m）\n",
    "        cache -- 反向传播需要的元组，包含了(a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    #1 使用tanh函数计算隐藏单元的激活值：a⟨t⟩=tanh(W_aa * a⟨t−1⟩+W_ax * x⟨t⟩+ba)\n",
    "    #2 使用a⟨t⟩计算                       yˆ⟨t⟩= softmax (W_ya* a⟨t⟩+by) \n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, x_t) + ba)\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)   \n",
    "    \n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, x_t, parameters)\n",
    "    return a_next, yt_pred, cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: rnn_forward\n",
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network \n",
    "参数：\n",
    "        x -- 输入的全部数据，维度为(n_x, m, T_x)\n",
    "        a0 -- 初始化隐藏状态，维度为 (n_a, m)\n",
    "        parameters -- 字典，包含了以下内容:\n",
    "                        Wax -- 矩阵，输入乘以权重，维度为（n_a, n_x）\n",
    "                        Waa -- 矩阵，隐藏状态乘以权重，维度为（n_a, n_a）\n",
    "                        Wya -- 矩阵，隐藏状态与输出相关的权重矩阵，维度为（n_y, n_a）\n",
    "                        ba  -- 偏置，维度为（n_a, 1）\n",
    "                        by  -- 偏置，隐藏状态与输出相关的偏置，维度为（n_y, 1）\n",
    "    返回：\n",
    "        a -- 所有时间步的隐藏状态，维度为(n_a, m, T_x)\n",
    "        y_pred -- 所有时间步的预测，维度为(n_y, m, T_x)\n",
    "        caches -- 为反向传播的保存的元组，维度为（【列表类型】cache, x)）\n",
    "    \"\"\"\n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []\n",
    "    # Retrieve dimensions from shapes of x and Wy\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    # initialize \"a\" and \"y\" with zeros \n",
    "    a = np.zeros([n_a, m, T_x])\n",
    "    y_pred = np.zeros([n_y, m, T_x])\n",
    "    \n",
    "    # Initialize a_next \n",
    "    a_next = a0\n",
    "    \n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache \n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a \n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y \n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        # Append \"cache\" to \"caches\" (≈1 line)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    return a, y_pred, caches\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
    "print(\"a[4][1] = \", a[4][1])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
    "print(\"y_pred.shape = \", y_pred.shape)\n",
    "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
    "print(\"len(caches) = \", len(caches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dx\"][1][2] = [-2.07101689 -0.59255627  0.02466855  0.01483317]\ngradients[\"dx\"].shape = (3, 10, 4)\ngradients[\"da0\"][2][3] = -0.31494237512664996\ngradients[\"da0\"].shape = (5, 10)\ngradients[\"dWax\"][3][1] = 11.264104496527777\ngradients[\"dWax\"].shape = (5, 3)\ngradients[\"dWaa\"][1][2] = 2.303333126579893\ngradients[\"dWaa\"].shape = (5, 5)\ngradients[\"dba\"][4] = [-0.74747722]\ngradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "#循环神经网络的反向传播\n",
    "def rnn_cell_backward(da_next, cache):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        da_next -- 关于下一个隐藏状态的损失的梯度。\n",
    "        cache -- 字典类型，rnn_step_forward()的输出\n",
    "    返回：\n",
    "        gradients -- 字典，包含了以下参数：\n",
    "                        dx -- 输入数据的梯度，维度为(n_x, m)\n",
    "                        da_prev -- 上一隐藏层的隐藏状态，维度为(n_a, m)\n",
    "                        dWax -- 输入到隐藏状态的权重的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 隐藏状态到隐藏状态的权重的梯度，维度为(n_a, n_a)\n",
    "                        dba -- 偏置向量的梯度，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    # Retrieve values from cache\n",
    "    (a_next, a_prev, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve values from parameters\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    # compute the gradient of tanh with respect to a_next \n",
    "    dtanh = (1-a_next * a_next) * da_next  \n",
    "\n",
    "    # compute the gradient of the loss with respect to Wax\n",
    "    dxt = np.dot(Wax.T,dtanh)\n",
    "    dWax = np.dot(dtanh, xt.T)\n",
    "\n",
    "    # compute the gradient with respect to Waa \n",
    "    da_prev = np.dot(Waa.T,dtanh)\n",
    "    dWaa = np.dot(dtanh, a_prev.T)\n",
    "\n",
    "    # compute the gradient with respect to b\n",
    "    dba = np.sum(dtanh, keepdims=True, axis=-1)\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rnn_backward(da, caches):\n",
    "    \"\"\"\n",
    "    在整个输入数据序列上实现RNN的反向传播\n",
    "    参数：\n",
    "        da -- 所有隐藏状态的梯度，维度为(n_a, m, T_x)\n",
    "        caches -- 包含向前传播的信息的元组\n",
    "    \n",
    "    返回：    \n",
    "        gradients -- 包含了梯度的字典：\n",
    "                        dx -- 关于输入数据的梯度，维度为(n_x, m, T_x)\n",
    "                        da0 -- 关于初始化隐藏状态的梯度，维度为(n_a, m)\n",
    "                        dWax -- 关于输入权重的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 关于隐藏状态的权值的梯度，维度为(n_a, n_a)\n",
    "                        dba -- 关于偏置的梯度，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    # Retrieve values from the first cache (t=1) of caches \n",
    "    (caches, x) = caches\n",
    "    (a1, a0, x1, parameters) = caches[0]\n",
    "    \n",
    "    # Retrieve dimensions from da's and x1's shapes \n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # initialize the gradients with the right sizes\n",
    "    dx = np.zeros([n_x, m, T_x])\n",
    "    dWax = np.zeros([n_a, n_x])\n",
    "    dWaa = np.zeros([n_a, n_a])\n",
    "    dba = np.zeros([n_a, 1])\n",
    "    da0 = np.zeros([n_a, m])\n",
    "    da_prevt = np.zeros([n_a, m])\n",
    "    \n",
    "    # Loop through all the time steps\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step. (≈1 line)\n",
    "        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])\n",
    "        # Retrieve derivatives from gradients \n",
    "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n",
    "        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)\n",
    "        dx[:, :, t] = dxt\n",
    "        dWax += dWaxt\n",
    "        dWaa += dWaat\n",
    "        dba += dbat\n",
    "        \n",
    "    # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) \n",
    "    da0 = da_prevt\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "a, y, caches = rnn_forward(x, a0, parameters)\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = rnn_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][3][6] =  0.17211776753291672\na.shape =  (5, 10, 7)\ny[1][4][3] = 0.9508734618501101\ny.shape =  (2, 10, 7)\ncaches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n  0.41005165]\nc[1][2][1] -0.8555449167181981\nlen(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "# 长短时记忆（Long Short-Term Memory (LSTM)）网络\n",
    "#      遗忘门forget:     Γf⟨t⟩= σ(W_f[a ⟨t−1⟩,x ⟨t⟩]+b_f )\n",
    "   #     输出门output:   Γo⟨t⟩= σ(W_o[a ⟨t−1⟩,x ⟨t⟩]+b_o )\n",
    " #   更新门update:       Γu⟨t⟩= σ(W_u[a ⟨t−1⟩,x ⟨t⟩]+b_u)\n",
    "#          更新单元：   c_~⟨t⟩= tanh (W_c * [a_⟨t−1⟩ ,x ⟨t⟩ ] + b_c)\n",
    "#       cell momory:    c_⟨t⟩= u⟨t⟩∗ c~⟨t⟩ + Γf⟨t⟩∗c_⟨t−1⟩ +\n",
    " # 激活单元activation:   a_⟨t⟩=Γo⟨t⟩ ∗ tanh(c_⟨t⟩)\n",
    "def lstm_cell_forward(x_t, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        x_t -- 在时间步“t”输入的数据，维度为(n_x, m)\n",
    "        a_prev -- 上一个时间步“t-1”的隐藏状态，维度为(n_a, m)\n",
    "        c_prev -- 上一个时间步“t-1”的记忆状态，维度为(n_a, m)\n",
    "        parameters -- 字典类型的变量，包含了：\n",
    "                        Wf -- 遗忘门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bf -- 遗忘门的偏置，维度为(n_a, 1)\n",
    "                        Wi -- 更新门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bi -- 更新门的偏置，维度为(n_a, 1)\n",
    "                        Wc -- 第一个“tanh”的权值，维度为(n_a, n_a + n_x)\n",
    "                        bc -- 第一个“tanh”的偏置，维度为(n_a, n_a + n_x)\n",
    "                        Wo -- 输出门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bo -- 输出门的偏置，维度为(n_a, 1)\n",
    "                        Wy -- 隐藏状态与输出相关的权值，维度为(n_y, n_a)\n",
    "                        by -- 隐藏状态与输出相关的偏置，维度为(n_y, 1)\n",
    "    返回：\n",
    "        a_next -- 下一个隐藏状态，维度为(n_a, m)\n",
    "        c_next -- 下一个记忆状态，维度为(n_a, m)\n",
    "        yt_pred -- 在时间步“t”的预测，维度为(n_y, m)\n",
    "        cache -- 包含了反向传播所需要的参数，包含了(a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "        \n",
    "    注意：\n",
    "        ft/it/ot表示遗忘/更新/输出门，cct表示候选值(c tilda)，c表示记忆值。\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # Retrieve dimensions from shapes of xt and Wy\n",
    "    n_x, m = x_t.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    # Concatenate a_prev and xt \n",
    "    concat = np.zeros([n_a + n_x, m])\n",
    "    concat[: n_a, :] = a_prev\n",
    "    concat[n_a :, :] = x_t\n",
    "\n",
    "    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)\n",
    "    #      遗忘门forget: Γf⟨t⟩= σ(W_f[a ⟨t−1⟩,x ⟨t⟩]+b_f )\n",
    "   #     输出门output:   Γo⟨t⟩= σ(W_o[a ⟨t−1⟩,x ⟨t⟩]+b_o )\n",
    " #   更新门update:       Γu⟨t⟩= σ(W_u[a ⟨t−1⟩,x ⟨t⟩]+b_u)\n",
    "#          更新单元：   c_~⟨t⟩= tanh (W_c * [a_⟨t−1⟩ ,x ⟨t⟩ ] + b_c)\n",
    "#       cell momory:    c_⟨t⟩= u⟨t⟩∗ c~⟨t⟩ + Γf⟨t⟩∗c_⟨t−1⟩ +\n",
    " # 激活单元activation:   a_⟨t⟩=Γo⟨t⟩ ∗ tanh(c_⟨t⟩)\n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf)\n",
    "    it = sigmoid(np.dot(Wi, concat) + bi)\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)\n",
    "    c_next = ft * c_prev + it * cct\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo)\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "    \n",
    "    # Compute prediction of the LSTM cell\n",
    "    yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, x_t, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: lstm_forward\n",
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "   实现LSTM单元组成的的循环神经网络\n",
    "    参数：\n",
    "        x -- 所有时间步的输入数据，维度为(n_x, m, T_x)\n",
    "        a0 -- 初始化隐藏状态，维度为(n_a, m)\n",
    "        parameters -- python字典，包含了以下参数：\n",
    "                        Wf -- 遗忘门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bf -- 遗忘门的偏置，维度为(n_a, 1)\n",
    "                        Wi -- 更新门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bi -- 更新门的偏置，维度为(n_a, 1)\n",
    "                        Wc -- 第一个“tanh”的权值，维度为(n_a, n_a + n_x)\n",
    "                        bc -- 第一个“tanh”的偏置，维度为(n_a, n_a + n_x)\n",
    "                        Wo -- 输出门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bo -- 输出门的偏置，维度为(n_a, 1)\n",
    "                        Wy -- 隐藏状态与输出相关的权值，维度为(n_y, n_a)\n",
    "                        by -- 隐藏状态与输出相关的偏置，维度为(n_y, 1)\n",
    "        \n",
    "    返回：\n",
    "        a -- 所有时间步的隐藏状态，维度为(n_a, m, T_x)\n",
    "        y -- 所有时间步的预测值，维度为(n_y, m, T_x)\n",
    "        caches -- 为反向传播的保存的元组，维度为（【列表类型】cache, x)）\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of xt and Wy \n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters['Wy'].shape\n",
    "    \n",
    "    # initialize \"a\", \"c\" and \"y\" with zeros \n",
    "    a = np.zeros([n_a, m, T_x])\n",
    "    c = np.zeros([n_a, m, T_x])\n",
    "    y = np.zeros([n_y, m, T_x])\n",
    "    \n",
    "    # Initialize a_next and c_next (≈2 lines)\n",
    "    a_next = a0\n",
    "    c_next = np.zeros([n_a, m])\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(x[:, :, t], a_next, c_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y \n",
    "        y[:,:,t] = yt\n",
    "        # Save the value of the next cell state \n",
    "        c[:,:,t]  = c_next\n",
    "        # Append the cache into caches \n",
    "        caches.append(cache)\n",
    "        \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "print(\"a[4][3][6] = \", a[4][3][6])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y[1][4][3] =\", y[1][4][3])\n",
    "print(\"y.shape = \", y.shape)\n",
    "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
    "print(\"c[1][2][1]\", c[1][2][1])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM反向传播\n",
    "# GRADED FUNCTION: lstm_cell_forward\n",
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        da_next -- 下一个隐藏状态的梯度，维度为(n_a, m)\n",
    "        dc_next -- 下一个单元状态的梯度，维度为(n_a, m)\n",
    "        cache -- 来自前向传播的一些参数\n",
    "        \n",
    "    返回：\n",
    "        gradients -- 包含了梯度信息的字典：\n",
    "                        dxt -- 输入数据的梯度，维度为(n_x, m)\n",
    "                        da_prev -- 先前的隐藏状态的梯度，维度为(n_a, m)\n",
    "                        dc_prev -- 前的记忆状态的梯度，维度为(n_a, m, T_x)\n",
    "                        dWf -- 遗忘门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbf -- 遗忘门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWi -- 更新门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbi -- 更新门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWc -- 第一个“tanh”的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbc -- 第一个“tanh”的偏置的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dWo -- 输出门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbo -- 输出门的偏置的梯度，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    # Retrieve information from \"cache\"\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from xt's and a_next's shape (≈2 lines)\n",
    "    n_x, m = xt.shape\n",
    "    n_a, m = a_next.shape\n",
    "    \n",
    "    # Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)\n",
    "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dcct = (dc_next * it + ot * (1 - np.square(np.tanh(c_next))) * it * da_next) * (1 - np.square(cct))\n",
    "    dit = (dc_next * cct + ot * (1 - np.square(np.tanh(c_next))) * cct * da_next) * it * (1 - it)\n",
    "    dft = (dc_next * c_prev + ot * (1 - np.square(np.tanh(c_next))) * c_prev * da_next) * ft * (1 - ft)\n",
    "    \n",
    "    # Compute parameters related derivatives. Use equations (11)-(14) \n",
    "    concat = np.concatenate((a_prev, xt), axis=0).T\n",
    "    dWf = np.dot(dft, concat)\n",
    "    dWi = np.dot(dit, concat)\n",
    "    dWc = np.dot(dcct, concat)\n",
    "    dWo = np.dot(dot, concat)\n",
    "    dbf = np.sum(dft,axis=1,keepdims=True)  \n",
    "    dbi = np.sum(dit,axis=1,keepdims=True)  \n",
    "    dbc = np.sum(dcct,axis=1,keepdims=True)  \n",
    "    dbo = np.sum(dot,axis=1,keepdims=True)  \n",
    "\n",
    "    # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines)\n",
    "    da_prev = np.dot(parameters[\"Wf\"][:, :n_a].T, dft) + np.dot(parameters[\"Wc\"][:, :n_a].T, dcct) + np.dot(parameters[\"Wi\"][:, :n_a].T, dit) + np.dot(parameters[\"Wo\"][:, :n_a].T, dot)\n",
    "    dc_prev = dc_next*ft+ot*(1-np.square(np.tanh(c_next)))*ft*da_next\n",
    "    dxt = np.dot(parameters[\"Wf\"][:, n_a:].T, dft) + np.dot(parameters[\"Wc\"][:, n_a:].T, dcct) + np.dot(parameters[\"Wi\"][:, n_a:].T, dit) + np.dot(parameters[\"Wo\"][:, n_a:].T, dot)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Save gradients in dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lstm_backward(da, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).\n",
    "\n",
    "    Arguments:\n",
    "    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)\n",
    "    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)\n",
    "    caches -- cache storing information from the forward pass (lstm_forward)\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradient of inputs, of shape (n_x, m, T_x)\n",
    "                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve values from the first cache (t=1) of caches.\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # initialize the gradients with the right sizes (≈12 lines)\n",
    "    dx = np.zeros([n_x, m, T_x])\n",
    "    da0 = np.zeros([n_a, m])\n",
    "    da_prevt = np.zeros([n_a, m])\n",
    "    dc_prevt = np.zeros([n_a, m])\n",
    "    dWf = np.zeros([n_a, n_a + n_x])\n",
    "    dWi = np.zeros([n_a, n_a + n_x])\n",
    "    dWc = np.zeros([n_a, n_a + n_x])\n",
    "    dWo = np.zeros([n_a, n_a + n_x])\n",
    "    dbf = np.zeros([n_a, 1])\n",
    "    dbi = np.zeros([n_a, 1])\n",
    "    dbc = np.zeros([n_a, 1])\n",
    "    dbo = np.zeros([n_a, 1])\n",
    "    \n",
    "    # loop back over the whole sequence\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Compute all gradients using lstm_cell_backward\n",
    "        gradients = lstm_cell_backward(da[:,:,t],dc_prevt,caches[t])\n",
    "        # da_prevt, dc_prevt = gradients['da_prev'], gradients[\"dc_prev\"]\n",
    "        # Store or add the gradient to the parameters' previous step's gradient\n",
    "        dx[:,:,t] = gradients['dxt']\n",
    "        dWf = dWf+gradients['dWf']\n",
    "        dWi = dWi+gradients['dWi']\n",
    "        dWc = dWc+gradients['dWc']\n",
    "        dWo = dWo+gradients['dWo']\n",
    "        dbf = dbf+gradients['dbf']\n",
    "        dbi = dbi+gradients['dbi']\n",
    "        dbc = dbc+gradients['dbc']\n",
    "        dbo = dbo+gradients['dbo']\n",
    "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
    "    da0 = gradients['da_prev']\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = lstm_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
