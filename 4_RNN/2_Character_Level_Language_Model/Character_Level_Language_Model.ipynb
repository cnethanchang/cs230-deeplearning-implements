{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append( os.path.abspath(\"../../workspace/python-work/python 3/cs230_deeplearning_implement/ec_code/\")  )\n",
    "from class5.week1.dinosaurus_island.cllm_utils import *\n",
    "\n",
    "#字符级语言模型 - 恐龙岛\n",
    "#已经收集了所有恐龙名字，并编入了这个数据集,\n",
    "# 为了构建字符级语言模型来生成新的名称，模型将学习不同的名称模式，并随机生成新的名字。\n",
    "\n",
    "#构建模型中的模块，将来构建整个模型中的两个重要的模块：\n",
    "# 梯度修剪：避免梯度爆炸\n",
    "# 取样:一种用来产生字符的技术\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open(r'C:\\workspace\\python-work\\python 3\\cs230_deeplearning_implement\\ec_code\\class5\\week1\\dinosaurus_island\\dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# 这些字符是a-z（26个英文字符）加上“\\n”（换行字符），在这里换行字符起到\n",
    "# 了在视频中类似的EOS（句子结尾）的作用，这里表示了名字的结束而不是句子的结尾。\n",
    "# 下面将创建一个字典，每个字符映射到0-26的索引，然后再创建一个字典，\n",
    "# 它将该字典将每个索引映射回相应的字符字符，它会帮助我们找出softmax层的概率分布输出中的字符。\n",
    "# 我们来创建char_to_ix 与 ix_to_char字典。\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度修剪：避免梯度爆炸\n",
    "#整个循环结构通常包括前向传播、成本计算、反向传播和参数更新。\n",
    "# 在更新参数之前，我们将在需要时执行梯度修剪，以确保我们的梯度不是“爆炸”的。\n",
    "#梯度修剪：梯度向量的每一个元素都被限制在[−N，N] [-N，N][−N，N]的范围，\n",
    "#       通俗的说，有一个maxValue（比如10），如果梯度的任何值大于10，那么它将被设置为10，\n",
    "#       如果梯度的任何值小于-10，那么它将被设置为-10，如果它在-10与10之间，那么它将不变。\n",
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. \n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    return gradients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: sample   \n",
    "# 采样  ==============>zero to many\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "    Arguments:\n",
    "    parameters --dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix --dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    # 从parameters 中获取参数\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]   #by.shape =>  (n_y,m)  这个model中n_y=n_x\n",
    "    n_a = Waa.shape[1]         #Waa.shape ==> (n_a,n_a)\n",
    "    # 创建独热向量x\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # 使用0初始化a_prev\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    # 创建索引的空列表，这是包含要生成的字符的索引的列表。\n",
    "    indices = []\n",
    "    # IDX是检测换行符的标志，我们将其初始化为-1。\n",
    "    idx = -1\n",
    "    # 循环遍历时间步骤t。在每个时间步中，从概率分布中抽取一个字符，\n",
    "    # 并将其索引附加到“indices”上，如果我们达到50个字符，\n",
    "    #（我们应该不太可能有一个训练好的模型），我们将停止循环，这有助于调试并防止进入无限循环\n",
    "    counter = 0\n",
    "    newline_character = char_to_ix[\"\\n\"]\n",
    "    \n",
    "    while (idx != newline_character and counter < 50):\n",
    "          # 计算隐藏单元的激活值：a⟨t⟩=tanh(W_aa * a⟨t−1⟩+W_ax * x⟨t⟩+ba)       (n_a,m)\n",
    "          #2计算              yˆ⟨t⟩= softmax (W_ya* a⟨t⟩+by)                 (n_y,m)\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        # 设定随机种子\n",
    "        np.random.seed(counter + seed)\n",
    "        # 从概率分布y中抽取词汇表中字符的索引\n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "          #index 是int，是y中概率最大数的索引\n",
    "        # 添加到索引中\n",
    "        indices.append(idx)\n",
    "        # 步骤4:将输入字符重写为与采样索引对应的字符。\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1\n",
    "        # 更新a_prev为a\n",
    "        a_prev = a \n",
    "        # 累加器\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "   #表示 名称的末尾\n",
    "    if(counter == 50):\n",
    "        indices.append(char_to_ix[\"\\n\"])\n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#gd梯度下降的一次迭代\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    执行训练模型的单步优化。\n",
    "    \n",
    "    参数：\n",
    "        X -- 整数列表，其中每个整数映射到词汇表中的字符。\n",
    "        Y -- 整数列表，与X完全相同，但向左移动了一个索引。\n",
    "        a_prev -- 上一个隐藏状态\n",
    "        parameters -- 字典，包含了以下参数：\n",
    "                        Wax -- 权重矩阵乘以输入，维度为(n_a, n_x)\n",
    "                        Waa -- 权重矩阵乘以隐藏状态，维度为(n_a, n_a)\n",
    "                        Wya -- 隐藏状态与输出相关的权重矩阵，维度为(n_y, n_a)\n",
    "                        b -- 偏置，维度为(n_a, 1)\n",
    "                        by -- 隐藏状态与输出相关的权重偏置，维度为(n_y, 1)\n",
    "        learning_rate -- 模型学习的速率\n",
    "    \n",
    "    返回：\n",
    "        loss -- 损失函数的值（交叉熵损失）\n",
    "        gradients -- 字典，包含了以下参数：\n",
    "                        dWax -- 输入到隐藏的权值的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 隐藏到隐藏的权值的梯度，维度为(n_a, n_a)\n",
    "                        dWya -- 隐藏到输出的权值的梯度，维度为(n_y, n_a)\n",
    "                        db -- 偏置的梯度，维度为(n_a, 1)\n",
    "                        dby -- 输出偏置向量的梯度，维度为(n_y, 1)\n",
    "        a[len(X)-1] -- 最后的隐藏状态，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    # 前向传播\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    # 反向传播\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    # 梯度修剪，[-5 , 5]\n",
    "    gradients = clip(gradients,5)\n",
    "    # 更新参数\n",
    "    parameters =update_parameters(parameters,gradients,learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, \n",
    "          dino_names = 7, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    训练模型并生成恐龙名字\n",
    "    参数：\n",
    "        data -- 语料库\n",
    "        ix_to_char -- 索引映射字符字典\n",
    "        char_to_ix -- 字符映射索引字典\n",
    "        num_iterations -- 迭代次数\n",
    "        n_a -- RNN单元数量\n",
    "        dino_names -- 每次迭代中采样的数量\n",
    "        vocab_size -- 在文本中的唯一字符的数量\n",
    "    返回：\n",
    "        parameters -- 学习后了的参数\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    with open(r\"C:\\workspace\\python-work\\python 3\\cs230_deeplearning_implement\\ec_code\\class5\\week1\\dinosaurus_island\\dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        # 定义一个训练样本\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        #每次训练一个样本=======================》随机梯度下降\n",
    "        # Perform one optimization step:\n",
    "        # Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        # 使用延迟来保持损失平滑,这是为了加速训练。\n",
    "        loss = smooth(loss, curr_loss)\n",
    "        \n",
    "        # 每2000次迭代，通过sample()生成“\\n”字符，检查模型是否学习正确\n",
    "        if j % 2000 == 0:\n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                # 采样 no to many  生成名字\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                #================important=====================\n",
    "                #================important=====================\n",
    "                #================important=====================\n",
    "                #================important=====================\n",
    "                #================important=====================\n",
    "                # 为了得到相同的效果，随机种子+1\n",
    "                seed += 1\n",
    "            \n",
    "            print(\"\\n\")\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.097224\n\nNkzxwtdmfqoeyhsqwasjjjvu\nKneb\nKzxwtdmfqoeyhsqwasjjjvu\nNeb\nZxwtdmfqoeyhsqwasjjjvu\nEb\nXwtdmfqoeyhsqwasjjjvu\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2000, Loss: 27.888091\n\nMivrosaurus\nInda\nIvtos\nMacaisjeeansaurosopeeyalos\nXusedolopaurus\nCa\nTos\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4000, Loss: 25.632304\n\nNiussianesaurus\nInedaisilachusatorsaurus\nIvusceolosaurus\nNecalpsaurus\nWusndonosaurus\nCaaeson\nTosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6000, Loss: 24.802620\n\nNgytosaurus\nInda\nKytosaurus\nNda\nXusmangosaurus\nCa\nTosaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8000, Loss: 24.252629\n\nOnyusanichomurosaurus\nKida\nLyus\nOla\nXusagnchukuqosaurus\nCa\nTrohichus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10000, Loss: 23.627444\n\nMivuselichojylosaurus\nKlecalosaurus\nLyuskdleosaurus\nMecalosaurus\nXussangosaurus\nDabeosaurus\nToremaronykusaerasaurcisaurus\n\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 12000, Loss: 23.391712\n\nMixusteoosaurus\nInecalosaurus\nJytosaurus\nMeeakosaurus\nXusteopoolus\nCabatron\nTrohilosaurus\n\n\n"
     ]
    }
   ],
   "source": [
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
